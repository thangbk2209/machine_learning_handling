sliding_encoders = [18,24]
sliding_decoders = [2,4]
sliding_inferences = [8,10]
batch_size_arr = [8]
num_units_LSTM_arr = [[16,4],[32,4],[64,4]]
dropout_rate = [0.95]
# activation for inference and decoder layer : - 1 is sigmoid
#                                              - 2 is relu
#                                              - 3 is tanh
#                                              - 4 is elu
activation= [1,2]
# 1: momentum
# 2: adam
# 3: rmsprop

optimizers = [2,3]

learning_rate = 0.005
epochs_encoder_decoder = 2000
epochs_inference = 2000
patience = 20  #number of epoch checking for early stopping
# num_units_LSTM_arr - array number units lstm for encoder and decoder

num_units_inference_arr = [[64],[128]]
